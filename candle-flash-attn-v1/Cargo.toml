[package]
name = "candle-flash-attn-v1"
description = "Flash attention V1 layer for the candle ML framework."
homepage = "https://github.com/huggingface/candle-extensions/candle-flash-attn-v1/"
documentation = "https://docs.rs/candle-flash-attn-v1"
readme = "README.md"
version.workspace = true
edition.workspace = true
keywords.workspace = true
categories.workspace  = true
license.workspace = true
repository.workspace = true

[dependencies]
candle = { workspace = true }
half = { workspace = true }

[build-dependencies]
anyhow = { workspace = true }
num_cpus = { workspace = true }
rayon = { workspace = true }

[dev-dependencies]
anyhow = { workspace = true }
candle-nn = { workspace = true }
